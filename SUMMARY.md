At the beginning of the project, I used Claude AI primarily as a brainstorming tool. I provided it with the requirements outlined on the Final Project webpage, and although its responses mostly restated the content I supplied, the additional detail it offered helped me generate new directions for the project. From this initial exchange, I identified several features I wanted to implement: linking between tasks, searching through notes, maintaining a list of tags, sorting tasks by creation date and deadlines, marking tasks as important, adding subtasks, and enabling task deletion. This ideation process helped me form a clear sense of the projectâ€™s scope before I began writing any code.
When I started generating code, I used GitHub Copilot in VS Code, limiting my use to the Agent and Ask modes. My first prompt asked Copilot to create a command-line application capable of storing, listing, and searching tasks in a JSON file. This prompt produced the foundational structure of my project. After the main code, Copilot automatically generated testing code, even though I had not explicitly requested it. These tests used the unittest framework at first, but once the course required the use of Pytest in tasks3, I had to ask Copilot to convert them. I also had to direct it to use standard assert statements at the end of the test functions.
Some of the early AI-generated features worked technically but were not practical for users. For example, Copilot assigned long, random alphanumeric strings as task IDs. These were accurate but inconvenient, especially when a user needed to reference or copy an ID manually. In response, I prompted Copilot to shorten IDs to eight characters and allow user-defined IDs. A similar issue occurred with date and time formatting. Copilot initially produced a format that was functional but difficult to read. After prompting, it switched the formatting to a more accessible UTC representation.
Throughout the development process, the README.md and docstrings/comments were not always updated along with the functional code. I would always prompt for updates to the documentation separately to be safe, and there were many times when Copilot had missed something. The errors would often be in filenames or references, or there would be missing examples for usage.
Apart from Pytest and UnitTest, I used Copilot Chat and Claude Chat for debugging and help with command line usage. When Pytest returned a test failure, Copilot was largely able to find and fix the problem. The only issue it could not figure out was with user input. Before tasks4, I attempted to implement user input and tab/autocomplete for the program. The program would get hung up after entering the input, and Copilot provided several fixes that did not work. Until I reached tasks4 with OpenAI chat completions, I was unable to incorporate this input prompting. 
When I ran into issues with Git/Github that the class notes did not cover, I asked Claude Chat for help. I also used Claude to get answers quickly, even if they were covered by the course notes. There were some issues with git push/pull and deletion that I ran into, and Claude provided different methods for fixing the problems I had. Tasks 1-5 all had separate branches from the main, so I needed to run push and pull with certain flags/arguments.
As I progressed, I discovered that using GitHub Copilot Agentic Chat worked best when I asked for one feature or patch at a time. This approach allowed Copilot to generate cleaner patches and enabled me to test each addition in isolation. Fine-grained Git commits further supported this workflow, as I could easily revert to a working version when something broke. By relying on both AI-generated tests and my own program usage, I was able to verify functional correctness, legibility, and user experience. Overall, there were only a few things that did not turn out how I wanted. I was unable to implement tab/autocomplete and command history in the final version. Additionally, I found that SpecKit added more extensive documentation than I needed and made updates more cumbersome. However, SpecKit works well when handling large sets of detailed specifications -- unlike Copilot Chat which struggled more when prompted for many highly detailed features at once. When using Copilot in this manner, output took longer to generate and required more work to check for errors.
